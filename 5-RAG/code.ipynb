{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a11d5e",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation\n",
    "`Query -> Search a Database -> Relevant Documents -> Send to LLM -> Contextually Relevant Answer` <br/>\n",
    "\n",
    "Complexity from decisions based on:\n",
    "- Chunking.\n",
    "- Databases.\n",
    "- Preprocessing query.\n",
    "- Postprocessing results.\n",
    "- Semantic vs Keywords.\n",
    "- Hypothetical searches.\n",
    "- Multi-hop retrieval.\n",
    "- Agentic retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758ae69",
   "metadata": {},
   "source": [
    "#### Multi-Hop Retrieval\n",
    "`Question -> LM <-> Hybrid Search from DB` <br/>\n",
    "`Context -> LM <-> DB` <br/>\n",
    "`Context -> LM -> Answer` <br/>\n",
    "\n",
    "#### Hybrid HyDE Search\n",
    "`Question -> HyDE LM -> (Semantic Query -> Embedding Search) + (BM-25 Query -> BM-25 Search) -> Reciprocal Rank Fusion`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f4530",
   "metadata": {},
   "source": [
    "### Setup Jokes DB\n",
    "<a href=\"https://www.kaggle.com/datasets/abhinavmoudgil95/short-jokes\">Dataset link.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4880984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haider\\Desktop\\code\\Context-Engineering\\with DSPy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def embed_texts(texts):\n",
    "    encoded_input = tokenizer(texts, padding=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    embeddings = model_output.last_hidden_state[:,0,:].numpy()\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b6b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path('embeddings.npy').exists():\n",
    "    data = pd.read_csv('shortjokes.csv')\n",
    "    jokes = data['Joke'].values\n",
    "    jokes = jokes[:5000]\n",
    "    \n",
    "    batch_size = 512\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(jokes), batch_size), desc='Generating embeddings'):\n",
    "        batch_texts = jokes[i:i+batch_size].tolist()\n",
    "        batch_embeddings = embed_texts(batch_texts)\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "\n",
    "    embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    print(f'Total embeddings: {len(embeddings)}')\n",
    "    np.save('embeddings.npy', embeddings)\n",
    "    with open('jokes.txt', 'w') as f:\n",
    "        for joke in jokes:\n",
    "            f.write(joke+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d80096",
   "metadata": {},
   "source": [
    "### Basic Nearest-Neighbors RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b12b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicEmbeddingsRAG:\n",
    "    def __init__(self, texts, embeddings):\n",
    "        self.texts = texts\n",
    "        self.embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    def get_nearest(self, query: str, k: int = 10):\n",
    "        query_emb = embed_texts([query])\n",
    "        query_emb = query_emb / np.linalg.norm(query_emb, axis=1, keepdims=True)\n",
    "        \n",
    "        # cosine similarity\n",
    "        # only need dot-product as the embeddings are already normalized\n",
    "        similarity = np.dot(query_emb, self.embeddings.T).flatten()\n",
    "        \n",
    "        topk_idxs = np.argpartition(similarity, -k)[-k:]\n",
    "        topk_idxs = sorted(topk_idxs, key=lambda x: similarity[x],\n",
    "                           reverse=True)\n",
    "        \n",
    "        return [self.texts[i] for i in topk_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a595edcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.042990922927856445\n",
      "[\"The best joke you'll never hear\", 'Meet the parents', 'Hire The Pretty Blonde', 'Just one time I wanna see The Bachelor get a cold sore', 'What do you call a bald porcupine? Pointless!', 'pull my upvote', \"My life That's the joke.\", 'What do you call corn with a sense of humor? Laughing stalk', 'What do you call a bald porcupine? Pointless.', 'Velcro. What a rip off!']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "query = 'Laugh'\n",
    "with open('jokes.txt', 'r') as f:\n",
    "    jokes = [l.strip() for l in f.readlines()]\n",
    "embs = np.load('embeddings.npy')\n",
    "\n",
    "basic_rag = BasicEmbeddingsRAG(jokes, embs)\n",
    "\n",
    "start = time.time()\n",
    "nearest = basic_rag.get_nearest(query, k=10)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Time: {end - start}')\n",
    "print(nearest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b4f59",
   "metadata": {},
   "source": [
    "### Approximate Nearest-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039b486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "class AnnoyRAG:\n",
    "    def __init__(self, texts, embeddings, n_trees=10):\n",
    "        self.texts = texts\n",
    "        self.emb_dim = embeddings.shape[1]\n",
    "        self.index = AnnoyIndex(self.emb_dim, 'angular')\n",
    "        \n",
    "        normalized_embs = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        for i, vec in enumerate(normalized_embs):\n",
    "            self.index.add_item(i, vec)\n",
    "        self.index.build(n_trees)\n",
    "    \n",
    "    def get_nearest(self, query: str, k: int = 10):\n",
    "        query_emb = embed_texts([query])\n",
    "        query_emb = query_emb / np.linalg.norm(query_emb, axis=1, keepdims=True)\n",
    "        \n",
    "        nearest_idxs = self.index.get_nns_by_vector(query_emb[0], k)\n",
    "        return [self.texts[i] for i in nearest_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb8e3390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for Basic: 0.018012285232543945\n",
      "Time for Annoy: 0.01598978042602539\n",
      "['What comes before OP? QWERTYUI', 'Be alert! The world needs more lerts.', '\"Blinding Nemo\" #BPMovies', 'How do you call a beautiful feminist? An oxymoron', 'Who is the king of the pencil case? The Ruler', 'Political Joke The Economy', '\"I see people.\" - The Fifth Sense', \"What comes after America? Bmerica. I'll see myself out\", 'Genderfluid? I just call that semen', 'Meet the parents']\n",
      "['Be alert! The world needs more lerts.', '\"Blinding Nemo\" #BPMovies', 'How do you call a beautiful feminist? An oxymoron', 'Political Joke The Economy', '\"I see people.\" - The Fifth Sense', 'Genderfluid? I just call that semen', 'Meet the parents', 'Velcro. What a rip off!', 'What is it that is yours , but others use it more than you ? Your name', \"What do you call someone incapable of eating people? A can't-ibal\"]\n"
     ]
    }
   ],
   "source": [
    "query = 'AI is rogue'\n",
    "\n",
    "basic_rag = BasicEmbeddingsRAG(jokes, embs)\n",
    "annoy_rag = AnnoyRAG(jokes, embs)\n",
    "\n",
    "start = time.time()\n",
    "nearest_basic = basic_rag.get_nearest(query, k=10)\n",
    "end = time.time()\n",
    "print(f'Time for Basic: {end - start}')\n",
    "\n",
    "start = time.time()\n",
    "nearest_annoy = annoy_rag.get_nearest(query, k=10)\n",
    "end = time.time()\n",
    "print(f'Time for Annoy: {end - start}')\n",
    "\n",
    "print(nearest_basic)\n",
    "print(nearest_annoy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17426a0b",
   "metadata": {},
   "source": [
    "### BM-25 Retrieval\n",
    "- Previous approaches are semantic-based.\n",
    "  - Uses embeddings.\n",
    "  - Captures overall semantic correlation.\n",
    "  - May mess up direct matches.\n",
    "- BM25 is keyword-based retrieval.\n",
    "  - Direct term-frequency matching.\n",
    "  - Can't capture synonyms, only direct matches.\n",
    "- E.g. Usecase: Searching for a specific model in a refrigerator manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "307b4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        self.bm25 = BM25Okapi([t.split(' ') for t in texts])  # tokenize by splitting on space\n",
    "    \n",
    "    def get_nearest(self, query: str, k: int = 10):\n",
    "        tokenized = query.split(' ')\n",
    "        topk_docs = self.bm25.get_top_n(tokenized, self.texts, n=k)\n",
    "        return topk_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "683ef719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.0019989013671875\n",
      "['What Cell Phone Company does Usain Bolt use? Sprint', 'Ever since the news came out about Samsung.... Their phones have been blowing up.', \"I bet kangaroos get tired of holding all of their friend's keys and cell phones while they're at the beach.\", 'I become instantly beautiful when I put on my sunglasses. -Every girl, ever.', 'What did the ruler gain a reputation for while campaigning? Straight talk.', 'How do you fit 4 gays on one barstool? Flip it over!', 'I want my tombstone to read \"Free WiFi\" so people would visit more often', 'You ever notice that the most dangerous thing about marijuana is getting caught with it?', 'What did Arnold Schwarzenegger say at the abortion clinic? Hasta last vista, baby.', 'Sucks that these Crest strips only come in white']\n"
     ]
    }
   ],
   "source": [
    "query = 'Cell phones'\n",
    "\n",
    "bm25_retriever = BM25Retriever(jokes)\n",
    "\n",
    "start = time.time()\n",
    "nearest_bm25 = bm25_retriever.get_nearest(query, k=10)\n",
    "end = time.time()\n",
    "print(f'Time: {end-start}')\n",
    "print(nearest_bm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9400e",
   "metadata": {},
   "source": [
    "### Combining Both Approaches\n",
    "#### Reciprocal Rank Fusion\n",
    "- Classic search engine technique.\n",
    "- Combines multiple ranked results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a46c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
    "    scores = {}\n",
    "    # calculate RRF scores\n",
    "    for ranked_list in ranked_lists:\n",
    "        for rank, doc in enumerate(ranked_list):\n",
    "            if doc not in scores:\n",
    "                scores[doc] = 0\n",
    "            scores[doc] += 1 / (k + rank + 1)\n",
    "    \n",
    "    docs = sorted(scores.keys(),\n",
    "                  key=lambda doc: scores[doc],\n",
    "                  reverse=True)\n",
    "    docs = docs[:k]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d946d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Results (0.0169s)\n",
      "\t1. Meet the parents\n",
      "\t2. South Africa\n",
      "\t3. pull my upvote\n",
      "\t4. Political Joke The Economy\n",
      "\t5. I have a joke to tell. Can you reddit?\n",
      "\t6. My life That's the joke.\n",
      "\t7. The best joke you'll never hear\n",
      "\t8. I like the sound of you not talking.\n",
      "\t9. Hire The Pretty Blonde\n",
      "\t10. I have a joke about Ebola You probably won't get it\n",
      "BM25 Results (0.0030s)\n",
      "\t1. What Cell Phone Company does Usain Bolt use? Sprint\n",
      "\t2. Ever since the news came out about Samsung.... Their phones have been blowing up.\n",
      "\t3. I bet kangaroos get tired of holding all of their friend's keys and cell phones while they're at the beach.\n",
      "\t4. I become instantly beautiful when I put on my sunglasses. -Every girl, ever.\n",
      "\t5. What did the ruler gain a reputation for while campaigning? Straight talk.\n",
      "\t6. How do you fit 4 gays on one barstool? Flip it over!\n",
      "\t7. I want my tombstone to read \"Free WiFi\" so people would visit more often\n",
      "\t8. You ever notice that the most dangerous thing about marijuana is getting caught with it?\n",
      "\t9. What did Arnold Schwarzenegger say at the abortion clinic? Hasta last vista, baby.\n",
      "\t10. Sucks that these Crest strips only come in white\n",
      "Fused and Re-ranked Results (Top 10)\n",
      "\t1. Meet the parents\n",
      "\t2. What Cell Phone Company does Usain Bolt use? Sprint\n",
      "\t3. South Africa\n",
      "\t4. Ever since the news came out about Samsung.... Their phones have been blowing up.\n",
      "\t5. pull my upvote\n",
      "\t6. I bet kangaroos get tired of holding all of their friend's keys and cell phones while they're at the beach.\n",
      "\t7. Political Joke The Economy\n",
      "\t8. I become instantly beautiful when I put on my sunglasses. -Every girl, ever.\n",
      "\t9. I have a joke to tell. Can you reddit?\n",
      "\t10. What did the ruler gain a reputation for while campaigning? Straight talk.\n"
     ]
    }
   ],
   "source": [
    "query = 'Cell phones'\n",
    "topk = 10\n",
    "\n",
    "vector_rag = BasicEmbeddingsRAG(jokes, embs)\n",
    "bm25_retriever = BM25Retriever(jokes)\n",
    "\n",
    "start = time.time()\n",
    "vector_results = vector_rag.get_nearest(query, k=topk)\n",
    "end = time.time()\n",
    "vector_time = end - start\n",
    "\n",
    "start = time.time()\n",
    "bm25_results = bm25_retriever.get_nearest(query, k=topk)\n",
    "end = time.time()\n",
    "bm25_time = end - start\n",
    "\n",
    "print(f'Vector Results ({vector_time:.4f}s)')\n",
    "for i, res in enumerate(vector_results):\n",
    "    print(f'\\t{i+1}. {res}')\n",
    "\n",
    "print(f'BM25 Results ({bm25_time:.4f}s)')\n",
    "for i, res in enumerate(bm25_results):\n",
    "    print(f'\\t{i+1}. {res}')\n",
    "\n",
    "fused_results = reciprocal_rank_fusion([vector_results, bm25_results])\n",
    "print(f'Fused and Re-ranked Results (Top {topk})')\n",
    "for i, res in enumerate(fused_results[:topk]):\n",
    "    print(f'\\t{i+1}. {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920bc1d",
   "metadata": {},
   "source": [
    "### Multi-Hop HyDE\n",
    "- Separate queries for Semantic and Keyword searches for maximum flexibility.\n",
    "  - Semantic search is optimized for Cosine Similarity search.\n",
    "  - BM25 search is optimized for short, keyword-based queries.\n",
    "- Multi-hop gives the LLM more chances to tune the query for a better hit.\n",
    "  - Often paired with validation checks for stopping earlier.\n",
    "  - E.g. Checking if the answer is already retrieved in a Q/A system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eab82eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from typing import Optional, List\n",
    "\n",
    "class HypotheticalDoc(dspy.Signature):\n",
    "    \"\"\"Given a query, generate hypothetical documents to search a database of one-liner jokes.\"\"\"\n",
    "    query: str = dspy.InputField(desc='User wants to fetch jokes related to this topic.')\n",
    "    retrieved_jokes: Optional[List[str]] = dspy.InputField(desc='Jokes previously retrieved from the DB. Use these to further tune your search.')\n",
    "    hypothetical_bm25_query: str = dspy.OutputField(desc='Sentence to query to retrieve more jokes about the query from the DB.')\n",
    "    hypothetical_semantic_query: str = dspy.OutputField(desc='Sentence to search with Cosine Similarity.')\n",
    "\n",
    "class MultiHopeHyDESearch(dspy.Module):\n",
    "    def __init__(self, texts, embs, n_hops=3, k=10):\n",
    "        self.pred = dspy.ChainOfThought(HypotheticalDoc)\n",
    "        self.pred.set_lm(dspy.LM('gemini/gemini-2.5-flash-lite'))\n",
    "        \n",
    "        self.emb_retriever = BasicEmbeddingsRAG(texts, embs)\n",
    "        self.bm25_retriever = BM25Retriever(texts)\n",
    "        \n",
    "        self.n_hops = n_hops\n",
    "        self.k = k\n",
    "    \n",
    "    def forward(self, query):\n",
    "        retrieved_jokes = []\n",
    "        all_jokes = []\n",
    "        for _ in range(self.n_hops):\n",
    "            new_query = self.pred(query=query, retrieved_jokes=retrieved_jokes)\n",
    "            print(new_query)\n",
    "            \n",
    "            emb_lists = self.emb_retriever.get_nearest(new_query.hypothetical_semantic_query)\n",
    "            bm25_lists = self.bm25_retriever.get_nearest(new_query.hypothetical_bm25_query)\n",
    "            retrieved_jokes = reciprocal_rank_fusion([emb_lists, bm25_lists], k=self.k)\n",
    "            all_jokes.extend(retrieved_jokes)\n",
    "        return dspy.Prediction(jokes=all_jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07bd93fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning='The user is asking for jokes about cell phones. Since no jokes have been retrieved yet, I should generate a broad BM25 query and a semantic query that captures the essence of cell phone jokes.',\n",
      "    hypothetical_bm25_query='cell phone jokes',\n",
      "    hypothetical_semantic_query='Jokes about mobile phones and their use.'\n",
      ")\n",
      "Prediction(\n",
      "    reasoning='The user is asking for jokes about cell phones. The retrieved jokes include one directly about cell phones (\"My cell phone is so nervous whenever I go to the countryside... ...it\\'s constantly on EDGE.\") and another that mentions cell phone providers (\"NEVER date someone that works for your cell phone provider. You\\'re welcome.\"). To find more jokes, I should focus on keywords related to cell phones, mobile phones, smartphones, and common cell phone-related scenarios or features.',\n",
      "    hypothetical_bm25_query='cell phone jokes, mobile phone humor, smartphone jokes, funny cell phone stories',\n",
      "    hypothetical_semantic_query='Jokes about the everyday use, features, and frustrations of cell phones and smartphones.'\n",
      ")\n",
      "Prediction(\n",
      "    reasoning='The user is asking for jokes about \"cell phones\". The retrieved jokes include one directly about cell phones (\"My cell phone is so nervous whenever I go to the countryside... ...it\\'s constantly on EDGE.\") and another that mentions cell phone providers. The other jokes are not related to cell phones. To find more relevant jokes, I should focus on keywords like \"cell phone\", \"phone\", \"mobile\", \"texting\", \"apps\", \"smartphone\", and related concepts.',\n",
      "    hypothetical_bm25_query='cell phone joke OR phone joke OR mobile joke OR smartphone joke OR texting joke',\n",
      "    hypothetical_semantic_query='Jokes about cell phones, smartphones, and mobile devices.'\n",
      ")\n",
      "['Inside jokes are bitterly resented by the homeless.', 'What kind of jokes do bad comedians tell their audience? Bad jokes.', \"NEVER date someone that works for your cell phone provider. You're welcome.\", \"Friends don't force friends to watch 'funny' YouTube videos.\", \"My cell phone is so nervous whenever I go to the countryside... ...it's constantly on EDGE.\", \"Friends don't force friends to watch 'funny' YouTube videos.\", \"NEVER date someone that works for your cell phone provider. You're welcome.\", 'What kind of jokes do bad comedians tell their audience? Bad jokes.', \"My cell phone is so nervous whenever I go to the countryside... ...it's constantly on EDGE.\", \"An introvert looks down at his own shoes. An extrovert looks at other people's shoes.\", \"Friends don't force friends to watch 'funny' YouTube videos.\", 'Most suitable joke for reddit [deleted]', \"An introvert looks down at his own shoes. An extrovert looks at other people's shoes.\", \"The best joke you'll never hear\", 'What kind of jokes do bad comedians tell their audience? Bad jokes.']\n"
     ]
    }
   ],
   "source": [
    "query = 'Cell phones'\n",
    "k = 5\n",
    "n_hops = 3\n",
    "\n",
    "hyde = MultiHopeHyDESearch(jokes, embs, n_hops, k)\n",
    "retrieved_jokes = hyde(query=query).jokes\n",
    "print(retrieved_jokes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b45f5c",
   "metadata": {},
   "source": [
    "### JokeGenerator Example\n",
    "`Query -> (Idea LM <-> WebSearch) -> Joke Idea -> (Joke LM <-> Joke DB) -> Joke`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ac4e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class JokeIdea(BaseModel):\n",
    "    setup: str\n",
    "    contradiction: str\n",
    "    punchline: str\n",
    "\n",
    "class QueryToIdea(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are a funny comedian and your goal is to generate a nice structure for a joke.\n",
    "    You are given sample punchlines from diverse topics, which can be used to make your own jokes about the specific query.\n",
    "    \"\"\"\n",
    "    query: str = dspy.InputField(desc='The theme of the joke')\n",
    "    joke_idea: JokeIdea = dspy.OutputField()\n",
    "\n",
    "class JokeJudge(dspy.Signature):\n",
    "    \"\"\"Rank each joke idea between 1 to N. Rank 1 is the most unique and funniest.\"\"\"\n",
    "    joke_idea: List[JokeIdea] = dspy.InputField()\n",
    "    joke_ratings: List[int] = dspy.OutputField(desc='Rank between 1, 2, 3, ..., N.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "631b2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "def check_score_goodness(args, pred):\n",
    "    n_samples = len(args['joke_idea'])\n",
    "    same_len = len(pred.joke_ratings) == n_samples\n",
    "    all_ranks_present = all([\n",
    "        (i+1) in pred.joke_ratings\n",
    "        for i in range(n_samples)\n",
    "    ])\n",
    "    return 1 if (same_len and all_ranks_present) else 0\n",
    "\n",
    "\n",
    "tavily_client = TavilyClient(api_key=os.getenv('TAVILY_API_KEY'))\n",
    "def fetch_recent_news(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Inputs a query string, searches for news, and returns the top results.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = tavily_client.search(query=query, topic='news', max_results=4)\n",
    "    return [\n",
    "        x['content']\n",
    "        for x in response['results']\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9eb77097",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = 'gemini/gemma-3-27b-it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2777f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "class IdeaGenerator(dspy.Module):\n",
    "    def __init__(self, n_samples=3):\n",
    "        self.query2idea = dspy.ReAct(\n",
    "            QueryToIdea,\n",
    "            [fetch_recent_news],\n",
    "            max_iters=1,\n",
    "        )\n",
    "        self.query2idea.set_lm(dspy.LM(LLM, temperature=1))\n",
    "        \n",
    "        self.judge = dspy.Refine(\n",
    "            dspy.ChainOfThought(JokeJudge),\n",
    "            N=3, reward_fn=check_score_goodness,\n",
    "            threshold=1,\n",
    "        )\n",
    "        self.judge.set_lm(dspy.LM(LLM, temperature=1))\n",
    "        \n",
    "        self.n_samples = n_samples\n",
    "    \n",
    "    async def acall(self, query: str) -> JokeIdea:\n",
    "        joke_ideas = await asyncio.gather(*[\n",
    "            self.query2idea.acall(query=query)\n",
    "            for _ in range(self.n_samples)\n",
    "        ])\n",
    "        \n",
    "        scores = self.judge(joke_idea=joke_ideas).joke_ratings\n",
    "        best_idx = scores.index(1)\n",
    "        selected = joke_ideas[best_idx]\n",
    "        \n",
    "        return selected.joke_idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46915e5f",
   "metadata": {},
   "source": [
    "#### Generate Citations\n",
    "- Asking the LLM to cite its responses from the retrieved jokes.\n",
    "  - The `punchline_ids` below.\n",
    "- Makes the LLM pay attention to the retrived documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a635a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated\n",
    "class IdeaToJoke(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are a funny comedian who likes to tell stories before delivering a punchline.\n",
    "    You are always funny and act on the input joke idea.\n",
    "    You are also provided some punchlines from a joke DB - this is just to help you get some thematic ideas.\n",
    "    \"\"\"\n",
    "    joke_idea: JokeIdea = dspy.InputField()\n",
    "    punchlines: List[str] = dspy.InputField(desc='A list of punchlines from other jokes which you may take inspiration from.')\n",
    "    \n",
    "    punchline_ids: List[int] = dspy.OutputField(desc='Which punchline indexes you used for inspiration.')\n",
    "    plan: str = dspy.OutputField(desc='How you will use the punchlines and the joke idea together to form a joke.')\n",
    "    joke: str = dspy.OutputField(desc='The full joke delivery in the comedian\\'s voice.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26902bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokeGenerator(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.idea2joke = dspy.ChainOfThought(IdeaToJoke)\n",
    "        self.idea2joke.set_lm(dspy.LM(LLM, temperature=0.7))\n",
    "    \n",
    "    async def acall(self, joke_idea: JokeIdea, punchlines: List[str]):\n",
    "        joke = self.idea2joke(joke_idea=joke_idea, punchlines=punchlines)\n",
    "        return dspy.Prediction(\n",
    "            inspiration=[punchlines[idx] for idx in joke.punchline_ids],\n",
    "            plan=joke.plan,\n",
    "            joke=joke.joke,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bec3c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.configure(lm=dspy.LM(LLM), temperature=1)\n",
    "dspy.configure_cache(enable_disk_cache=False, enable_memory_cache=False)\n",
    "\n",
    "idea_gen = IdeaGenerator(n_samples=3)\n",
    "joke_gen = JokeGenerator()\n",
    "\n",
    "retriever = MultiHopeHyDESearch(jokes, embs, n_hops=2, k=5)\n",
    "retriever.pred.set_lm(dspy.LM(LLM))\n",
    "\n",
    "query = 'Cell phones'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ae7443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idea = await idea_gen.acall(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40949acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JokeIdea(setup=\"My nephew's school implemented a new cell phone policy this year. First offense, phone confiscated for the day. Second offense, confiscated for a week. Third offense...\", contradiction='They send the *parents* to detention.', punchline='Turns out, the parents are way more addicted to their phones.')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81634b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning=\"The user is looking for jokes about cell phones. The provided joke is a good example of a joke *about* cell phones, specifically focusing on addiction to them. Since no jokes were previously retrieved, I will formulate queries that aim to find similar jokes, focusing on cell phone addiction, usage, and related humorous situations. I'll create both a BM25 query (keyword-based) and a semantic query (meaning-based) to broaden the search.\",\n",
      "    hypothetical_bm25_query='\"cell phone addiction\" OR \"mobile phone addiction\" OR \"phone obsession\" OR \"cell phone humor\" OR \"mobile phone jokes\"',\n",
      "    hypothetical_semantic_query='\"jokes about people being overly reliant on their smartphones\"'\n",
      ")\n",
      "Prediction(\n",
      "    reasoning='The retrieved jokes include one directly about phones (\"My phone got arrested today... It was charged with battery.\") and others that are unrelated. The initial query was \"Cell phones\" and the setup/punchline provided also relate to cell phone addiction. To retrieve more relevant jokes, we should focus on queries that specifically mention cell phones, mobile phones, or phone-related humor. We can also broaden the search to include jokes about technology and communication.',\n",
      "    hypothetical_bm25_query='\"cell phone jokes\" OR \"mobile phone humor\" OR \"phone addiction jokes\"',\n",
      "    hypothetical_semantic_query='\"jokes about cell phones and technology\"'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "search_query = f\"\"\"\n",
    "query={query}\n",
    "setup={idea.setup}\n",
    "punchline={idea.punchline}\n",
    "\"\"\"\n",
    "\n",
    "punchlines = retriever(query=search_query).jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79afba83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How do you get to the front page of reddit ? Lack of originality',\n",
       " 'Yo mama so fat... ... slapping her belly causes gravitational waves. OR ... every step she takes causes a ripple in special relativity.',\n",
       " 'Warning: Joke contains racism Racism',\n",
       " 'My phone got arrested today... It was charged with battery.',\n",
       " 'Fun typo: \"You ate the most important thing in my life.\"',\n",
       " '\"Choas Theory\"-themed restaurant: Eating Disorder',\n",
       " 'Yo mama so fat... ... slapping her belly causes gravitational waves. OR ... every step she takes causes a ripple in special relativity.',\n",
       " 'Fun typo: \"You ate the most important thing in my life.\"',\n",
       " 'My phone got arrested today... It was charged with battery.',\n",
       " 'Warning: Joke contains racism Racism']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punchlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65f10a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke = await joke_gen.acall(joke_idea=idea, punchlines=punchlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4956eb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    inspiration=['Fun typo: \"You ate the most important thing in my life.\"'],\n",
       "    plan=\"I'll start by setting the scene – a parent-teacher conference. I'll exaggerate the seriousness of the school's new policy, then describe the increasingly frustrated parents being called in for their kids' offenses. I'll build the tension, hinting at something unusual happening, before finally revealing that *the parents* are the ones being punished with detention. The original punchline is perfect for this setup.\",\n",
       "    joke='(Adjusts mic, leans in conspiratorially) So, my nephew\\'s school, they\\'re cracking down on phones. Like, *serious* crackdown. First offense, phone confiscated for the day. Okay, fair enough. Second offense, a week. Still kinda harsh, but alright. But then… then comes the third offense. I went to a parent-teacher conference, right? And the principal is explaining this new policy, looking all stern. He\\'s saying, \"We\\'ve had to take drastic measures. Repeat offenders… well, it\\'s not the kids we\\'re sending to detention anymore.\" (Pauses for effect, looks around the room) Turns out, they send the *parents* to detention. Yeah, you heard me. Detention! Because, honestly, the parents are way more addicted to their phones. I mean, I saw Mrs. Henderson trying to sneak a peek at TikTok *during* detention! It\\'s a whole thing.'\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c2c25",
   "metadata": {},
   "source": [
    "### Memory - dspy.History\n",
    "- With `chat.completions`, we need to manage history and persistence across sessions ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7b6cf50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswering(dspy.Signature):\n",
    "    \"\"\"Respond to user's question.\"\"\"\n",
    "    question: str = dspy.InputField()\n",
    "    # remember conversation\n",
    "    history: dspy.History = dspy.InputField()\n",
    "    answer: str = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "627ec354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User: Hi, this is Mr. Memory.\n",
      "> AI: Hello Mr. Memory, it's nice to meet you! I'm ready when you are. Please ask me anything.\n",
      "> User: Tell me my name.\n",
      "> AI: Your name is Mr. Memory.\n"
     ]
    }
   ],
   "source": [
    "chat_module = dspy.ChainOfThought(QuestionAnswering)\n",
    "chat_module.set_lm(dspy.LM(LLM))\n",
    "\n",
    "questions = [\n",
    "    'Hi, this is Mr. Memory.',\n",
    "    'Tell me my name.'\n",
    "]\n",
    "\n",
    "history = dspy.History(messages=[])\n",
    "for q in questions:\n",
    "    print(f'> User: {q}')\n",
    "    \n",
    "    ans = chat_module(question=q, history=history)\n",
    "    print(f'> AI: {ans.answer}')\n",
    "    \n",
    "    # each message has the input and output keys from the signature\n",
    "    history.messages.extend([\n",
    "        {'n': q, **ans}\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd74049",
   "metadata": {},
   "source": [
    "#### Basic Persistence with Tools\n",
    "- File writing tools are excellent for an LLM to manage its own context.\n",
    "- Caveat: Handling them when the files get really large.\n",
    "\n",
    "#### Memory System Gotchas\n",
    "- Allow LLMs to read/write information.\n",
    "- Efficient retrieval when memory file is large.\n",
    "- Update/delete memories when they no longer relevant.\n",
    "- Manage session memory vs persistent memory.\n",
    "- Partition memory by users to prevent leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "694f1d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswering(dspy.Signature):\n",
    "    \"\"\"Respond to user's question. Write useful information about the user into a memory file.\"\"\"\n",
    "    question: str = dspy.InputField()\n",
    "    memory: str = dspy.InputField()\n",
    "    history: dspy.History = dspy.InputField()\n",
    "    answer: str = dspy.OutputField()\n",
    "\n",
    "def write_into_memory(markdown_text) -> str:\n",
    "    print(f'Writing into memory: {markdown_text}')\n",
    "    with open('memory.md', 'w') as f:\n",
    "        f.write(markdown_text)\n",
    "    return 'success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "60b9748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User: Hi, this is Mr. Memory.\n",
      "Writing into memory: User introduced themselves as Mr. Memory.\n",
      "> AI: Hello Mr. Memory! It's a pleasure to meet you. How can I help you today?\n",
      "> User: Tell me my name.\n",
      "> AI: Your name is Mr. Memory.\n"
     ]
    }
   ],
   "source": [
    "chat_module = dspy.ReAct(QuestionAnswering, [write_into_memory])\n",
    "chat_module.set_lm(dspy.LM(LLM))\n",
    "\n",
    "questions = [\n",
    "    'Hi, this is Mr. Memory.',\n",
    "    'Tell me my name.',\n",
    "]\n",
    "\n",
    "history = dspy.History(messages=[])\n",
    "for q in questions:\n",
    "    if Path('memory.md').exists():\n",
    "        memory = open('memory.md', 'r').readlines()\n",
    "    else:\n",
    "        memory = '<No Memory>'\n",
    "    \n",
    "    print(f'> User: {q}')\n",
    "    \n",
    "    ans = chat_module(question=q, history=history, memory=memory)\n",
    "    print(f'> AI: {ans.answer}')\n",
    "    \n",
    "    history.messages.extend([\n",
    "        {'User question': q, **ans}\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7959f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> User: Tell me my name.\n"
     ]
    }
   ],
   "source": [
    "# now only ask this question\n",
    "questions = [\n",
    "    'Tell me my name.',\n",
    "]\n",
    "\n",
    "history = dspy.History(messages=[])\n",
    "for q in questions:\n",
    "    if Path('memory.md').exists():\n",
    "        memory = open('memory.md', 'r').readlines()\n",
    "    else:\n",
    "        memory = '<No Memory>'\n",
    "    \n",
    "    print(f'> User: {q}')\n",
    "    \n",
    "    ans = chat_module(question=q, history=history, memory=memory)\n",
    "    print(f'> AI: {ans.answer}')\n",
    "    \n",
    "    history.messages.extend([\n",
    "        {'User question': q, **ans}\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e511f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "with DSPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
