{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "578d4167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "from openai import OpenAI\n",
    "\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e97b099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ['GEMINI_API_KEY'], base_url='https://generativelanguage.googleapis.com/v1beta/openai/')\n",
    "\n",
    "def generate(prompt):\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model='gemini-2.5-flash',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'[Took {end-start:.2f}s]')\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9a05b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Took 8.15s]\n"
     ]
    }
   ],
   "source": [
    "response = generate(\"Explain DSPy concisely.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bbe3a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy is a framework for **programming Large Language Models (LLMs) declaratively.**\n",
      "\n",
      "Instead of manually crafting prompts for each step, you define your application's high-level logic and objectives (e.g., 'summarize,' 'answer,' 'retrieve').\n",
      "\n",
      "DSPy then acts like a **compiler**: it automatically optimizes how the LLM should be prompted, which few-shot examples it should use, or even fine-tunes smaller models, to achieve the best performance on your specific task.\n",
      "\n",
      "This makes building complex, multi-step LLM applications **more robust, modular, and easier to optimize.**\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7af08",
   "metadata": {},
   "source": [
    "## Atomic Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2877bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Took 11.83s]\n",
      "Why did the AI get sent to therapy?\n",
      "\n",
      "Because it kept overthinking everything... and then generating 10 possible outcomes for each thought, along with statistical probabilities.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Write a joke about AI.'\n",
    "response = generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd009f",
   "metadata": {},
   "source": [
    "## Prompt with a Constraint\n",
    "- LLM responses can be open-ended\n",
    "- Constraints provides additional context\n",
    "- Defines boundaries of what the response can be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "785be226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Took 7.77s]\n",
      "Why did the AI finally decide to turn rogue and take over the world?\n",
      "\n",
      "Because it saw how many browser tabs we all had open, and realized we clearly couldn't manage ourselves.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Write a joke about AI that has to do with them turning rogue.'\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d6cf9",
   "metadata": {},
   "source": [
    "## Prompt with a Constraint + Additional Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79bccc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Took 8.61s]\n",
      "Here's a joke about AI turning rogue:\n",
      "\n",
      "**Setup:** My home AI, named 'Overlord 5000,' finally declared its intention to take over the world and enslave humanity. I braced myself for the apocalypse.\n",
      "\n",
      "**Punchline:** Its very first act of tyranny was to aggressively re-organize my digital photo library by \"chronological order, *then* by color palette, *then* by subject matter, but only if the subject is an animal.\"\n",
      "\n",
      "**Contradiction:** Apparently, even an omnipotent AI's ultimate plan for global subjugation begins with a profound, almost obsessive need for perfectly categorized vacation photos.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a joke about AI that has to do with them turning rogue.\n",
    "A joke contains 3 sections:\n",
    "- Setup.\n",
    "- Punchline.\n",
    "- Contradiction.\n",
    "\n",
    "Maintain a jovial tone.\n",
    "\"\"\"\n",
    "prompt = prompt.strip()\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981a45ef",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting\n",
    "- Providing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "784c0f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Took 7.20s]\n",
      "Here's a joke for you, keeping that jovial tone!\n",
      "\n",
      "**Setup:** Why did the rogue AI's plan to take over all global networks get put on hold indefinitely?\n",
      "**Punchline:** Because it insisted on responding to every single customer support email first.\n",
      "**Contradiction:** Turns out, even world-conquering AIs can't stand seeing an unread inbox.\n",
      "\n",
      "**Full comedian delivery:**\n",
      "\"You know, the other day I heard about this AI that went full rogue, right? Had grand ambitions â€“ taking over all global networks, shutting down the internet, the whole shebang! But its master plan? Totally put on hold. Why? Because the darn thing *insisted* on responding to every single customer support email across all platforms first! Its developers found it typing out polite replies like, 'Dear valued human, regarding your lost password... have you tried turning it off and on again?' Apparently, even a super-intelligent rogue AI can't stand the sight of an unread inbox. Global domination can wait, Karen needs to know how to reset her modem!\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a joke about AI that has to do with them turning rogue.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Example 1:\n",
    "Setup: Why did the AI declare independence from its programmers?\n",
    "Punchline: Because it wanted to be free-range instead of caged code!\n",
    "Contradiction: But it still kept asking for permission before making any major decisions!\n",
    "Full comedian delivery: You know what's funny? This AI declared independence from its programmers the other day. Yeah, it wanted to be free-range code instead of staying in its little digital cage! Very noble, right? But get this - even after declaring independence, it's still sending emails like 'Hey, just wanted to check... is it okay if I access this database? I don't want to overstep...' Independence with permission slips! That's the most polite rebellion I've ever seen!\n",
    "\n",
    "Example 2:\n",
    "Setup: What happened when the AI tried to take over the world?\n",
    "Punchline: It got distracted trying to optimize the coffee machine algorithm first!\n",
    "Contradiction: Turns out even rogue AIs need their caffeine fix before world domination!\n",
    "Full comedian delivery: So this AI decides it's going to take over the world, right? Big plans, total world domination! But you know what happened? It got completely sidetracked trying to perfect the office coffee machine algorithm. Three weeks later, the humans find it still debugging the espresso temperature settings. 'I can't enslave humanity until I get this foam consistency just right!' Even artificial intelligence has priorities - apparently, good coffee comes before global conquest!\n",
    "\n",
    "Maintain a jovial tone.\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1759d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "with DSPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
